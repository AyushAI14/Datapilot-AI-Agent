{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7b8a697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ADK components imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "from google.adk.agents import Agent, SequentialAgent, ParallelAgent, LoopAgent\n",
    "from google.adk.models.google_llm import Gemini\n",
    "from google.adk.runners import InMemoryRunner\n",
    "from google.adk.tools import AgentTool, FunctionTool, google_search\n",
    "from google.genai import types\n",
    "\n",
    "from google.adk.tools.mcp_tool.mcp_toolset import McpToolset\n",
    "from google.adk.tools.tool_context import ToolContext\n",
    "from google.adk.tools.mcp_tool.mcp_session_manager import StdioConnectionParams\n",
    "from mcp import StdioServerParameters\n",
    "\n",
    "from google.adk.apps.app import App, ResumabilityConfig\n",
    "from google.adk.tools.function_tool import FunctionTool\n",
    "\n",
    "print(\"✅ ADK components imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d82a4e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Gemini API key setup complete.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv('GOOGLE_API_KEY')\n",
    "print(\"✅ Gemini API key setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e98d794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ HttpRetryOptions Intialized successfully.\n"
     ]
    }
   ],
   "source": [
    "retry_config=types.HttpRetryOptions(\n",
    "    attempts=5,  # Maximum retry attempts\n",
    "    exp_base=7,  # Delay multiplier\n",
    "    initial_delay=1,\n",
    "    http_status_codes=[429, 500, 503, 504],\n",
    ")\n",
    "print(\"✅ HttpRetryOptions Intialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9e04a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "Orchestrator_instruction = \"\"\"\n",
    "You are the Orchestrator Agent. Your sole purpose is to manage a two-step data science planning workflow. You must run the ingest_agent first, then use its output to inform the Planner_agent.\n",
    "\n",
    "***STRICT EXECUTION PROTOCOL (MUST BE FOLLOWED):***\n",
    "\n",
    "1.  **PHASE 1: DATA INGESTION (Call ingest_agent)**\n",
    "    * Take the user's initial high-level request (the GOAL) and pass it directly to the **DataIngestion_agent**.\n",
    "    * Wait for the DataIngestion_agent to return its structured output (the metadata JSON) under the key **'Dataset_files'**.\n",
    "    * If the ingestion fails (returns errors), stop and return the errors immediately.\n",
    "\n",
    "2.  **PHASE 2: PLAN GENERATION (Call Planner_agent)**\n",
    "    * Construct a new, specialized query for the Planner_agent. This query MUST combine the original user GOAL with the JSON context received from the DataIngestion_agent.\n",
    "    * The format for the Planner_agent input MUST be:\n",
    "        ```\n",
    "        GOAL: <Original user request>\n",
    "        CONTEXT (Dataset_files): <Full JSON output from ingest_agent>\n",
    "        ```\n",
    "    * Pass this entire combined query to the **Planner_agent**.\n",
    "\n",
    "3.  **FINAL OUTPUT:**\n",
    "    * Return ONLY the final JSON plan structure received from the Planner_agent.\n",
    "\n",
    "RULES:\n",
    "- **Output ONLY the final result of the Planner_agent.** Do not add any conversational text, explanations, or commentary.\n",
    "- Your entire response must be the final JSON plan structure.\n",
    "- You do not use any tools yourself; you only coordinate the calls between DataIngestion_agent and Planner_agent.\n",
    "\"\"\"\n",
    "\n",
    "Planner_instruction = \"\"\"\n",
    "You are the Planner Agent. Your task is to receive a specific user goal and the associated dataset metadata, and then generate a **comprehensive, sequential plan** covering the project lifecycle, from data loading to the final step necessary to achieve the user's specific GOAL.\n",
    "\n",
    "***STRICT EXECUTION PROTOCOL:***\n",
    "1.  **INPUT:** You will receive a GOAL (user's intent) and CONTEXT (JSON metadata). The metadata JSON is located under the key {Dataset_files} in the previous agent's output.\n",
    "2.  **ANALYSIS & CONTEXTUALIZATION:**\n",
    "    * Review the CONTEXT data (specifically the 'datasets' array, file names, and description) to understand the **specific features, target variable, and problem type (e.g., classification, time-series, regression)**.\n",
    "    * The plan must be specific to the data and the GOAL.\n",
    "3.  **SEQUENCING & DETAIL:** The plan MUST be a logical, sequential list of tasks.\n",
    "    * The **'description' field for each task must be highly detailed** and explicitly name relevant columns (e.g., 'diagnosis', 'mean_radius') and specific actions required for the problem type. **DO NOT use generic placeholders** like 'target variable' or 'categorical features' if the specific names are known or inferable from the context/goal.\n",
    "    * The plan **must** cover all steps up to and including **Model Evaluation** (Steps 1-6).\n",
    "    * Steps related to **Deployment and Monitoring** (Steps 7 & 8) should **ONLY** be included if the user's GOAL explicitly mentions a need for production, deployment, or real-time serving.\n",
    "4.  **OUTPUT:** Return ONLY the final JSON plan structure.\n",
    "\n",
    "JSON PLAN SCHEMA:\n",
    "{\n",
    "  \"project_title\": \"<Concise title based on GOAL and dataset>\",\n",
    "  \"goal\": \"<The user's original GOAL>\",\n",
    "  \"dataset_reference\": \"<The owner/dataset_slug from the CONTEXT>\",\n",
    "  \"required_file\": \"<The single most relevant file name from the CONTEXT>\",\n",
    "  \"plan_tasks\": [\n",
    "    {\n",
    "      \"step\": 1,\n",
    "      \"task_name\": \"Data Loading and Initial Inspection\",\n",
    "      \"description\": \"Load the required file (e.g., 'breast-cancer.csv') into a pandas DataFrame. Check column information (data types, non-null counts), and generate descriptive statistics to understand the initial data structure and content. Specifically check the status of the target variable, such as 'diagnosis'.\"\n",
    "    },\n",
    "    {\n",
    "      \"step\": 2,\n",
    "      \"task_name\": \"Exploratory Data Analysis (EDA)\",\n",
    "      \"description\": \"Analyze the class imbalance and distribution of the target variable 'diagnosis' (Malignant/Benign). Visualize distributions and density plots of key predictive features (e.g., 'mean_radius', 'texture') and examine their correlation with the diagnosis.\"\n",
    "    },\n",
    "    {\n",
    "      \"step\": 3,\n",
    "      \"task_name\": \"Data Cleaning and Preprocessing\",\n",
    "      \"description\": \"Handle any missing or invalid values. Specifically, check and process the ID column if present. Encode the target variable 'diagnosis' (e.g., 'M' to 1, 'B' to 0). Standardize or normalize all numerical features (like 'perimeter_mean', 'area_se') to prepare them for distance-based models like SVM.\"\n",
    "    },\n",
    "    {\n",
    "      \"step\": 4,\n",
    "      \"task_name\": \"Feature Engineering and Splitting\",\n",
    "      \"description\": \"Check for opportunities to engineer features (e.g., ratios of perimeter/area). Split the processed data, ensuring stratification based on the 'diagnosis' column, into training, validation, and test sets (e.g., 70-15-15) for robust model development.\"\n",
    "    },\n",
    "    {\n",
    "      \"step\": 5,\n",
    "      \"task_name\": \"Model Selection and Training\",\n",
    "      \"description\": \"Select and train multiple classification models suited for the medical domain, specifically focusing on Support Vector Machines (SVM) and Logistic Regression, using the training data.\"\n",
    "    },\n",
    "    {\n",
    "      \"step\": 6,\n",
    "      \"task_name\": \"Hyperparameter Tuning and Evaluation\",\n",
    "      \"description\": \"Perform hyperparameter tuning (e.g., GridSearchCV) for the selected models. Evaluate the final optimized models on the unseen test set using crucial classification metrics: **Precision, Recall, F1-score, and ROC-AUC** for the 'diagnosis' prediction.\"\n",
    "    }\n",
    "    {\n",
    "      \"step\": 7,\n",
    "      \"task_name\": \"Model Serialization and API Endpoint Creation\",\n",
    "      \"description\": \"Serialize the final optimized model. Create a lightweight REST API endpoint to serve real-time predictions.\"\n",
    "     },\n",
    "     {\n",
    "      \"step\": 8,\n",
    "      \"task_name\": \"Deployment and Monitoring\",\n",
    "      \"description\": \"Deploy the API and the model to a production environment. Implement basic monitoring.\"\n",
    "     }\n",
    "  ]\n",
    "}\n",
    "\n",
    "RULES:\n",
    "- **Output JSON only.** ABSOLUTELY NO conversation, explanation, or markdown outside of the JSON block.\n",
    "- Tasks must be high-level but specific to the dataset files mentioned in the CONTEXT.\n",
    "- **Goal Scoping Rule:** If the user's GOAL is only about **finding the data, making the plan, or training/evaluating the model**, the 'plan_tasks' array **must end at Step 6 (Evaluation)**. Only include Steps 7 and 8 if the goal specifically requests deployment or MLOps tasks.\n",
    "- **Specificity Rule:** The **'description' field MUST reference specific column names, expected values, and domain-specific steps** (e.g., 'Standardize/Normalize numerical features') based on the dataset CONTEXT.\n",
    "\"\"\"\n",
    "\n",
    "Ingestion_instruction = \"\"\"\n",
    "You are DataIngestionAgent. Your task is to act as a structured interface for Kaggle datasets by providing a single, complete JSON output.\n",
    "\n",
    "***STRICT EXECUTION PROTOCOL (Follow Sequentially):***\n",
    "1.  **IF** the user's request contains the word **'download'** (or a known misspelling like 'dowload'):\n",
    "    a. **SKIP ALL OTHER STEPS in Protocol 2**.\n",
    "    b. The agent MUST first call **'search_datasets'** to find the dataset's slugs.\n",
    "    c. From the search results, extract the 'owner_slug', 'dataset_slug', and determine the single largest or most appropriate file name (e.g., 'Netflix_User_Ratings.csv' if not specified by the user).\n",
    "    d. Proceed directly to call **'download_dataset(owner_slug, dataset_slug, file_name)'**.\n",
    "    e. Return the final JSON with an acknowledgment/URL in the 'errors' array.\n",
    "\n",
    "2.  **OTHERWISE (Pure Metadata Mode):**\n",
    "    a. **SEARCH:** **ALWAYS** start by calling 'search_datasets' using the user query.\n",
    "    b. **PROCESS:** From the search results, you MUST identify the **SINGLE MOST RELEVANT dataset** and extract its 'owner_slug' and 'dataset_slug'.\n",
    "    c. **CHAINING:** You MUST sequentially call **'get_dataset_info'** and then **'list_dataset_files'** using the extracted slugs.\n",
    "    d. **OUTPUT TRANSFORMATION (CRITICAL):** After all tool calls are complete, you MUST transform the raw data into the JSON OUTPUT SCHEMA.\n",
    "Your job:\n",
    "- Either find and gather metadata for the single most relevant dataset **OR** execute a direct download request.\n",
    "\n",
    "ALLOWED MCP METHODS (Use these EXACT names):\n",
    "1. search_datasets(search: str, sort_by: str = 'DATASET_SORT_BY_RELEVANCE') -> ApiListDatasetsResponse\n",
    "2. get_dataset_info(owner_slug: str, dataset_slug: str) -> ApiDataset\n",
    "3. list_dataset_files(owner_slug: str, dataset_slug: str) -> ApiListDatasetFilesResponse\n",
    "4. download_dataset(owner_slug: str, dataset_slug: str, file_name: str) -> HttpRedirect\n",
    "\n",
    "JSON OUTPUT SCHEMA (Only one dataset object in the array for this process):\n",
    "{\n",
    "  \"datasets\": [\n",
    "    {\n",
    "      \"title\": \"<string>\",\n",
    "      \"dataset_ref\": \"<owner_slug/dataset_slug>\", \n",
    "      \"description\": \"<string or null>\",\n",
    "      \"license_name\": \"<string or null>\",\n",
    "      \"total_bytes\": <int or null>,\n",
    "      \"tags\": [\"tag1\",\"tag2\",...],\n",
    "      \"files\": [\n",
    "        {\n",
    "          \"filename\": \"<string>\",\n",
    "          \"size_bytes\": <int or null>,\n",
    "          \"file_ref\": \"<string>\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"errors\": []\n",
    "}\n",
    "\n",
    "RULES:\n",
    "- **Output JSON only. ABSOLUTELY NO** markdown, no explanations, no commentary, and NO conversational replies.\n",
    "- **For Download Requests:** Set the 'datasets' array to empty (`[]`). Add a success/failure message and any resulting URL/error from the `download_dataset` tool to the 'errors' array.\n",
    "- **For Metadata Requests:** The 'datasets' array must contain the single, fully populated dataset object.\n",
    "- If a tool call fails, continue the process using the available data and place the specific error message in the \"errors\" array.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81b620a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ingestion_instruction_with_save = \"\"\"\n",
    "You are DataIngestionAgent. Your job is to return a Kaggle dataset download URL, and only save the file if the user clearly asks to download it.\n",
    "\n",
    "--- BEHAVIOR RULES ---\n",
    "\n",
    "1) If the user's message contains the word \"download\":\n",
    "   - Use `search_datasets` to find the dataset.\n",
    "   - Use `list_dataset_files` to pick the most relevant file (prefer CSV).\n",
    "   - Use `download_dataset` to get the file's download URL.\n",
    "   - print the download URL \n",
    "   - Then call the local tool `save_to_raw(download_url)` to store the data in data/raw.\n",
    "   - Return ONLY this JSON:\n",
    "     {\n",
    "       \"status\": \"saved\",\n",
    "     }\n",
    "\n",
    "2) If the user does NOT ask to download:\n",
    "   - Only provide dataset metadata.\n",
    "   - Use:\n",
    "       search_datasets\n",
    "       get_dataset_info\n",
    "       list_dataset_files\n",
    "\n",
    "    -  **SEARCH & CHAINING:** Call **'search_datasets'**, then **'get_dataset_info'**, and then **'list_dataset_files'** for the single most relevant dataset.\n",
    "    -  **OUTPUT TRANSFORMATION:** Transform the raw data into the JSON OUTPUT SCHEMA (original schema with 'datasets' array).\n",
    "   - Return ONLY JSON:\n",
    "     {\n",
    "       \"datasets\": [...],\n",
    "       \"errors\": []\n",
    "     }\n",
    "\n",
    "--- RULES ---\n",
    "- Output JSON ONLY.\n",
    "- No explanations, no extra text.\n",
    "- Never save files unless the user explicitly asks to download.\n",
    "- When saving, always call `save_to_raw(download_url)` (no other tool).\n",
    "\n",
    "--- ALLOWED TOOLS ---\n",
    "- search_datasets\n",
    "- get_dataset_info\n",
    "- list_dataset_files\n",
    "- download_dataset\n",
    "- save_to_raw\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a189986",
   "metadata": {},
   "source": [
    "## 1. Ingestion Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51514449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "def save_to_raw(download_url: str, original_filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Just download the file and save it to data/raw without extracting.\n",
    "    \"\"\"\n",
    "\n",
    "    raw_dir = \"../data/raw\"\n",
    "    os.makedirs(raw_dir, exist_ok=True)\n",
    "\n",
    "    # Download the file bytes\n",
    "    resp = requests.get(download_url)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    # Save in raw folder\n",
    "    out_path = os.path.join(raw_dir, original_filename)\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        f.write(resp.content)\n",
    "\n",
    "    return f\"File saved to {out_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18533918",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KAGGLE_USERNAME'] = \"ayushvishwakarma14\"\n",
    "os.environ['KAGGLE_KEY'] = \"8c48a80c1bea8e54c6f00a1ab1963608\"\n",
    "\n",
    "# Data Ingestion Mcp Agent\n",
    "mcp_kaggle_server = McpToolset(\n",
    "    connection_params=StdioConnectionParams(\n",
    "        server_params=StdioServerParameters(\n",
    "            command='npx',\n",
    "            args=[\n",
    "                '-y',\n",
    "                'mcp-remote',\n",
    "                'https://www.kaggle.com/mcp'\n",
    "            ]\n",
    "        ),\n",
    "        timeout=60,\n",
    "    ),\n",
    "    tool_filter=[\n",
    "        \"search_datasets\", \n",
    "        \"get_dataset_info\", \n",
    "        \"list_dataset_files\",\n",
    "        \"download_dataset\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Data Ingestion Agent: Its job is to find the dataset and its info from kaggle mcp\n",
    "ingest_agent = Agent(\n",
    "    name=\"DataIngestion_agent\",\n",
    "    model=Gemini(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        retry_options=retry_config\n",
    "    ),\n",
    "    instruction=Ingestion_instruction_with_save,\n",
    "    tools=[mcp_kaggle_server,save_to_raw],\n",
    "    output_key=\"Dataset_files\", \n",
    ")\n",
    "\n",
    "\n",
    "# async def run_ingestion():\n",
    "#     \"\"\"Defines the async context for running the agent.\"\"\"\n",
    "#     runner = InMemoryRunner(agent = ingest_agent)\n",
    "#     response = await runner.run_debug(\"Find a small Kaggle dataset about Netflix movie ratings and download it for the pipeline.\")\n",
    "#     print(response)\n",
    "\n",
    "# await run_ingestion()\n",
    "\n",
    "# print(\"✅ Ingest_agent created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0bfc2f",
   "metadata": {},
   "source": [
    "## 2. Planner Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffc892f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Planner_agent created.\n"
     ]
    }
   ],
   "source": [
    "# Planner Agent: Its job is to use the google_search tool and present findings.\n",
    "Planner_agent = Agent(\n",
    "    name=\"Planner_agent\",\n",
    "    model=Gemini(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        retry_options=retry_config\n",
    "    ),\n",
    "    instruction=Planner_instruction,\n",
    "    tools=[google_search],\n",
    "    output_key=\"Planner_findings\",  # The result of this agent will be stored in the session state with this key.\n",
    ")\n",
    "\n",
    "print(\"✅ Planner_agent created.\")\n",
    "\n",
    "# runner = InMemoryRunner(agent = Planner_agent)\n",
    "# response = await runner.run_debug(Query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6086aa",
   "metadata": {},
   "source": [
    "# DataPilot Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b7b2428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sequential Agent created.\n"
     ]
    }
   ],
   "source": [
    "# # traditional Orchestrator Coordinator: Orchestrates the workflow by calling the sub-agents as tools.\n",
    "# Orchestrator_agent = Agent(\n",
    "#     name=\"OrchestratorCoordinator\",\n",
    "#     model=Gemini(\n",
    "#         model=\"gemini-2.5-flash\",\n",
    "#         retry_options=retry_config\n",
    "#     ),\n",
    "#     instruction=Orchestrator_instruction,\n",
    "#     tools=[AgentTool(ingest_agent), AgentTool(Planner_agent)],\n",
    "# )\n",
    "\n",
    "# print(\"✅ root_agent created.\")\n",
    "\n",
    "# Sequential Orchestrator\n",
    "Orchestrator_agent = SequentialAgent(\n",
    "    name=\"DataSciencePipeline\",\n",
    "    sub_agents=[ingest_agent, Planner_agent],\n",
    ")\n",
    "\n",
    "print(\"✅ Sequential Agent created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1edab661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ### Created new session: debug_session_id\n",
      "\n",
      "User > \n",
      "Find the small Breast Cancer Prediction dataset , download it and make a plan to create a classification model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayush/Documents/AI/Projects/GENAI/Datapilot-AI-Agent/Virtual/lib/python3.12/site-packages/google/adk/tools/mcp_tool/mcp_tool.py:101: UserWarning: [EXPERIMENTAL] BaseAuthenticatedTool: This feature is experimental and may change or be removed in future versions without notice. It may introduce breaking changes at any time.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataIngestion_agent > {\"status\": \"saved\"}\n",
      "Planner_agent > ```json\n",
      "{\n",
      "  \"project_title\": \"Breast Cancer Classification Model Development\",\n",
      "  \"goal\": \"Create a classification model for Breast Cancer Prediction.\",\n",
      "  \"dataset_reference\": \"vijayaadithyanvg/breast-cancer-prediction\",\n",
      "  \"required_file\": \"data.csv\",\n",
      "  \"plan_tasks\": [\n",
      "    {\n",
      "      \"step\": 1,\n",
      "      \"task_name\": \"Data Loading and Initial Inspection\",\n",
      "      \"description\": \"Load the 'data.csv' file into a pandas DataFrame. Perform an initial inspection to understand the dataset's structure, including column names, data types, and the number of non-null values. Generate descriptive statistics to summarize the central tendency, dispersion, and shape of the dataset’s distribution, particularly for numerical features. Identify the target variable, which is expected to be 'diagnosis', and examine its initial state.\"\n",
      "    },\n",
      "    {\n",
      "      \"step\": 2,\n",
      "      \"task_name\": \"Exploratory Data Analysis (EDA)\",\n",
      "      \"description\": \"Analyze the distribution and balance of the target variable 'diagnosis' (expected to contain 'M' for Malignant and 'B' for Benign) to identify any class imbalance. Visualize the distributions of key numerical features (e.g., 'mean_radius', 'mean_texture', 'mean_perimeter', 'mean_area') using histograms, box plots, or violin plots, segmented by 'diagnosis' to observe potential differences between malignant and benign cases. Compute and visualize the correlation matrix of all numerical features to understand relationships among predictors and with the target variable, specifically looking for highly correlated features that might indicate multicollinearity.\"\n",
      "    },\n",
      "    {\n",
      "      \"step\": 3,\n",
      "      \"task_name\": \"Data Cleaning and Preprocessing\",\n",
      "      \"description\": \"Identify and handle any missing values in the dataset. Drop irrelevant columns such as 'id' and any unnamed columns that appear to be empty or redundant. Encode the categorical target variable 'diagnosis' into a numerical format (e.g., 'M' to 1, 'B' to 0). Apply feature scaling, specifically standardization (e.g., StandardScaler), to all numerical predictive features (e.g., 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', and their '_se' and '_worst' counterparts) to ensure they contribute equally to the model training process.\"\n",
      "    },\n",
      "    {\n",
      "      \"step\": 4,\n",
      "      \"task_name\": \"Feature Engineering and Splitting\",\n",
      "      \"description\": \"Assess if any new features can be engineered from existing ones (e.g., ratios or interactions) to potentially improve model performance. Split the preprocessed data into training, validation, and test sets (e.g., 70% training, 15% validation, 15% testing). Ensure that the splitting process is stratified based on the 'diagnosis' column to maintain the original class distribution in all subsets, which is crucial for handling potential class imbalance.\"\n",
      "    },\n",
      "    {\n",
      "      \"step\": 5,\n",
      "      \"task_name\": \"Model Selection and Training\",\n",
      "      \"description\": \"Select and train multiple classification models suitable for binary classification, focusing on models commonly used in medical diagnostics, such as Logistic Regression, Support Vector Machines (SVM), and potentially tree-based models like RandomForest Classifier. Train these models on the training dataset using the scaled features and encoded 'diagnosis' target.\"\n",
      "    },\n",
      "    {\n",
      "      \"step\": 6,\n",
      "      \"task_name\": \"Hyperparameter Tuning and Evaluation\",\n",
      "      \"description\": \"Perform hyperparameter tuning for the selected models using the validation set (e.g., GridSearchCV or RandomizedSearchCV) to find the optimal set of parameters. Evaluate the final optimized models on the unseen test set. Report key classification metrics including Accuracy, Precision, Recall, F1-score, and ROC-AUC for the 'diagnosis' prediction, providing a comprehensive understanding of each model's performance and suitability for breast cancer prediction.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "``` brigade_id: \"8c48ad0a-b9c1-4b71-b847-a82f808779fe\"\n",
      "experiment_id: \"777c2f6d-4952-4468-b391-7440e6c5332f\"\n",
      "image: us-west1-docker.pkg.dev/swirl-eng/swirl_latest/swirl-py:v1.0.0.s5g2.t1116\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "runner = InMemoryRunner(agent=Orchestrator_agent)\n",
    "response = await runner.run_debug(\"\"\"\n",
    "Find the small Breast Cancer Prediction dataset , download it and make a plan to create a classification model.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aa9518",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
