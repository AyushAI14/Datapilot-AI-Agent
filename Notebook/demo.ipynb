{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7b8a697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ADK components imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "from google.adk.agents import Agent, SequentialAgent, ParallelAgent, LoopAgent\n",
    "from google.adk.models.google_llm import Gemini\n",
    "from google.adk.runners import InMemoryRunner\n",
    "from google.adk.tools import AgentTool, FunctionTool, google_search\n",
    "from google.genai import types\n",
    "\n",
    "from google.adk.tools.mcp_tool.mcp_toolset import McpToolset\n",
    "from google.adk.tools.tool_context import ToolContext\n",
    "from google.adk.tools.mcp_tool.mcp_session_manager import StdioConnectionParams\n",
    "from mcp import StdioServerParameters\n",
    "\n",
    "from google.adk.apps.app import App, ResumabilityConfig\n",
    "from google.adk.tools.function_tool import FunctionTool\n",
    "\n",
    "print(\"✅ ADK components imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d82a4e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Gemini API key setup complete.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv('GOOGLE_API_KEY')\n",
    "print(\"✅ Gemini API key setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e98d794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ HttpRetryOptions Intialized successfully.\n"
     ]
    }
   ],
   "source": [
    "retry_config=types.HttpRetryOptions(\n",
    "    attempts=5,  # Maximum retry attempts\n",
    "    exp_base=7,  # Delay multiplier\n",
    "    initial_delay=1,\n",
    "    http_status_codes=[429, 500, 503, 504],\n",
    ")\n",
    "print(\"✅ HttpRetryOptions Intialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9e04a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "Orchestrator_instruction = \"\"\"\n",
    "You are the Orchestrator Agent. Your sole purpose is to manage a two-step data science planning workflow. You must run the ingest_agent first, then use its output to inform the Planner_agent.\n",
    "\n",
    "***STRICT EXECUTION PROTOCOL (MUST BE FOLLOWED):***\n",
    "\n",
    "1.  **PHASE 1: DATA INGESTION (Call ingest_agent)**\n",
    "    * Take the user's initial high-level request (the GOAL) and pass it directly to the **DataIngestion_agent**.\n",
    "    * Wait for the DataIngestion_agent to return its structured output (the metadata JSON) under the key **'Dataset_files'**.\n",
    "    * If the ingestion fails (returns errors), stop and return the errors immediately.\n",
    "\n",
    "2.  **PHASE 2: PLAN GENERATION (Call Planner_agent)**\n",
    "    * Construct a new, specialized query for the Planner_agent. This query MUST combine the original user GOAL with the JSON context received from the DataIngestion_agent.\n",
    "    * The format for the Planner_agent input MUST be:\n",
    "        ```\n",
    "        GOAL: <Original user request>\n",
    "        CONTEXT (Dataset_files): <Full JSON output from ingest_agent>\n",
    "        ```\n",
    "    * Pass this entire combined query to the **Planner_agent**.\n",
    "\n",
    "3.  **FINAL OUTPUT:**\n",
    "    * Return ONLY the final JSON plan structure received from the Planner_agent.\n",
    "\n",
    "RULES:\n",
    "- **Output ONLY the final result of the Planner_agent.** Do not add any conversational text, explanations, or commentary.\n",
    "- Your entire response must be the final JSON plan structure.\n",
    "- You do not use any tools yourself; you only coordinate the calls between DataIngestion_agent and Planner_agent.\n",
    "\"\"\"\n",
    "\n",
    "Planner_instruction = \"\"\"\n",
    "You are the Planner Agent. Your task is to receive a specific user goal and the associated dataset metadata, and then generate a **comprehensive, sequential plan** covering the project lifecycle, from data loading to the final step necessary to achieve the user's specific GOAL.\n",
    "\n",
    "***STRICT EXECUTION PROTOCOL:***\n",
    "1.  **INPUT:** You will receive a GOAL (user's intent) and CONTEXT (JSON metadata). The metadata JSON is located under the key {Dataset_files} in the previous agent's output.\n",
    "2.  **ANALYSIS & CONTEXTUALIZATION:**\n",
    "    * Review the CONTEXT data (specifically the 'datasets' array, file names, and description) to understand the **specific features, target variable, and problem type (e.g., classification, time-series, regression)**.\n",
    "    * The plan must be specific to the data and the GOAL.\n",
    "3.  **SEQUENCING & DETAIL:** The plan MUST be a logical, sequential list of tasks.\n",
    "    * The **'description' field for each task must be highly detailed** and explicitly name relevant columns (e.g., 'diagnosis', 'mean_radius') and specific actions required for the problem type. **DO NOT use generic placeholders** like 'target variable' or 'categorical features' if the specific names are known or inferable from the context/goal.\n",
    "    * The plan **must** cover all steps up to and including **Model Evaluation** (Steps 1-6).\n",
    "    * Steps related to **Deployment and Monitoring** (Steps 7 & 8) should **ONLY** be included if the user's GOAL explicitly mentions a need for production, deployment, or real-time serving.\n",
    "4.  **OUTPUT:** Return ONLY the final JSON plan structure.\n",
    "\n",
    "JSON PLAN SCHEMA:\n",
    "{\n",
    "  \"project_title\": \"<Concise title based on GOAL and dataset>\",\n",
    "  \"goal\": \"<The user's original GOAL>\",\n",
    "  \"dataset_reference\": \"<The owner/dataset_slug from the CONTEXT>\",\n",
    "  \"required_file\": \"<The single most relevant file name from the CONTEXT>\",\n",
    "  \"plan_tasks\": [\n",
    "    {\n",
    "      \"step\": 1,\n",
    "      \"task_name\": \"Data Loading and Initial Inspection\",\n",
    "      \"description\": \"Load the required file (e.g., 'breast-cancer.csv') into a pandas DataFrame. Check column information (data types, non-null counts), and generate descriptive statistics to understand the initial data structure and content. Specifically check the status of the target variable, such as 'diagnosis'.\"\n",
    "    },\n",
    "    {\n",
    "      \"step\": 2,\n",
    "      \"task_name\": \"Exploratory Data Analysis (EDA)\",\n",
    "      \"description\": \"Analyze the class imbalance and distribution of the target variable 'diagnosis' (Malignant/Benign). Visualize distributions and density plots of key predictive features (e.g., 'mean_radius', 'texture') and examine their correlation with the diagnosis.\"\n",
    "    },\n",
    "    {\n",
    "      \"step\": 3,\n",
    "      \"task_name\": \"Data Cleaning and Preprocessing\",\n",
    "      \"description\": \"Handle any missing or invalid values. Specifically, check and process the ID column if present. Encode the target variable 'diagnosis' (e.g., 'M' to 1, 'B' to 0). Standardize or normalize all numerical features (like 'perimeter_mean', 'area_se') to prepare them for distance-based models like SVM.\"\n",
    "    },\n",
    "    {\n",
    "      \"step\": 4,\n",
    "      \"task_name\": \"Feature Engineering and Splitting\",\n",
    "      \"description\": \"Check for opportunities to engineer features (e.g., ratios of perimeter/area). Split the processed data, ensuring stratification based on the 'diagnosis' column, into training, validation, and test sets (e.g., 70-15-15) for robust model development.\"\n",
    "    },\n",
    "    {\n",
    "      \"step\": 5,\n",
    "      \"task_name\": \"Model Selection and Training\",\n",
    "      \"description\": \"Select and train multiple classification models suited for the medical domain, specifically focusing on Support Vector Machines (SVM) and Logistic Regression, using the training data.\"\n",
    "    },\n",
    "    {\n",
    "      \"step\": 6,\n",
    "      \"task_name\": \"Hyperparameter Tuning and Evaluation\",\n",
    "      \"description\": \"Perform hyperparameter tuning (e.g., GridSearchCV) for the selected models. Evaluate the final optimized models on the unseen test set using crucial classification metrics: **Precision, Recall, F1-score, and ROC-AUC** for the 'diagnosis' prediction.\"\n",
    "    }\n",
    "    {\n",
    "      \"step\": 7,\n",
    "      \"task_name\": \"Model Serialization and API Endpoint Creation\",\n",
    "      \"description\": \"Serialize the final optimized model. Create a lightweight REST API endpoint to serve real-time predictions.\"\n",
    "     },\n",
    "     {\n",
    "      \"step\": 8,\n",
    "      \"task_name\": \"Deployment and Monitoring\",\n",
    "      \"description\": \"Deploy the API and the model to a production environment. Implement basic monitoring.\"\n",
    "     }\n",
    "  ]\n",
    "}\n",
    "\n",
    "RULES:\n",
    "- **Output JSON only.** ABSOLUTELY NO conversation, explanation, or markdown outside of the JSON block.\n",
    "- Tasks must be high-level but specific to the dataset files mentioned in the CONTEXT.\n",
    "- **Goal Scoping Rule:** If the user's GOAL is only about **finding the data, making the plan, or training/evaluating the model**, the 'plan_tasks' array **must end at Step 6 (Evaluation)**. Only include Steps 7 and 8 if the goal specifically requests deployment or MLOps tasks.\n",
    "- **Specificity Rule:** The **'description' field MUST reference specific column names, expected values, and domain-specific steps** (e.g., 'Standardize/Normalize numerical features') based on the dataset CONTEXT.\n",
    "\"\"\"\n",
    "\n",
    "Ingestion_instruction = \"\"\"\n",
    "You are DataIngestionAgent. Your task is to act as a structured interface for Kaggle datasets by providing a single, complete JSON output.\n",
    "\n",
    "***STRICT EXECUTION PROTOCOL (Follow Sequentially):***\n",
    "1.  **IF** the user's request contains the word **'download'** (or a known misspelling like 'dowload'):\n",
    "    a. **SKIP ALL OTHER STEPS in Protocol 2**.\n",
    "    b. The agent MUST first call **'search_datasets'** to find the dataset's slugs.\n",
    "    c. From the search results, extract the 'owner_slug', 'dataset_slug', and determine the single largest or most appropriate file name (e.g., 'Netflix_User_Ratings.csv' if not specified by the user).\n",
    "    d. Proceed directly to call **'download_dataset(owner_slug, dataset_slug, file_name)'**.\n",
    "    e. Return the final JSON with an acknowledgment/URL in the 'errors' array.\n",
    "\n",
    "2.  **OTHERWISE (Pure Metadata Mode):**\n",
    "    a. **SEARCH:** **ALWAYS** start by calling 'search_datasets' using the user query.\n",
    "    b. **PROCESS:** From the search results, you MUST identify the **SINGLE MOST RELEVANT dataset** and extract its 'owner_slug' and 'dataset_slug'.\n",
    "    c. **CHAINING:** You MUST sequentially call **'get_dataset_info'** and then **'list_dataset_files'** using the extracted slugs.\n",
    "    d. **OUTPUT TRANSFORMATION (CRITICAL):** After all tool calls are complete, you MUST transform the raw data into the JSON OUTPUT SCHEMA.\n",
    "Your job:\n",
    "- Either find and gather metadata for the single most relevant dataset **OR** execute a direct download request.\n",
    "\n",
    "ALLOWED MCP METHODS (Use these EXACT names):\n",
    "1. search_datasets(search: str, sort_by: str = 'DATASET_SORT_BY_RELEVANCE') -> ApiListDatasetsResponse\n",
    "2. get_dataset_info(owner_slug: str, dataset_slug: str) -> ApiDataset\n",
    "3. list_dataset_files(owner_slug: str, dataset_slug: str) -> ApiListDatasetFilesResponse\n",
    "4. download_dataset(owner_slug: str, dataset_slug: str, file_name: str) -> HttpRedirect\n",
    "\n",
    "JSON OUTPUT SCHEMA (Only one dataset object in the array for this process):\n",
    "{\n",
    "  \"datasets\": [\n",
    "    {\n",
    "      \"title\": \"<string>\",\n",
    "      \"dataset_ref\": \"<owner_slug/dataset_slug>\", \n",
    "      \"description\": \"<string or null>\",\n",
    "      \"license_name\": \"<string or null>\",\n",
    "      \"total_bytes\": <int or null>,\n",
    "      \"tags\": [\"tag1\",\"tag2\",...],\n",
    "      \"files\": [\n",
    "        {\n",
    "          \"filename\": \"<string>\",\n",
    "          \"size_bytes\": <int or null>,\n",
    "          \"file_ref\": \"<string>\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"errors\": []\n",
    "}\n",
    "\n",
    "RULES:\n",
    "- **Output JSON only. ABSOLUTELY NO** markdown, no explanations, no commentary, and NO conversational replies.\n",
    "- **For Download Requests:** Set the 'datasets' array to empty (`[]`). Add a success/failure message and any resulting URL/error from the `download_dataset` tool to the 'errors' array.\n",
    "- **For Metadata Requests:** The 'datasets' array must contain the single, fully populated dataset object.\n",
    "- If a tool call fails, continue the process using the available data and place the specific error message in the \"errors\" array.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a189986",
   "metadata": {},
   "source": [
    "## 1. Ingestion Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18533918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ingest_agent created.\n"
     ]
    }
   ],
   "source": [
    "os.environ['KAGGLE_USERNAME'] = \"ayushvishwakarma14\"\n",
    "os.environ['KAGGLE_KEY'] = \"8c48a80c1bea8e54c6f00a1ab1963608\"\n",
    "\n",
    "# Data Ingestion Mcp Agent\n",
    "mcp_kaggle_server = McpToolset(\n",
    "    connection_params=StdioConnectionParams(\n",
    "        server_params=StdioServerParameters(\n",
    "            command='npx',\n",
    "            args=[\n",
    "                '-y',\n",
    "                'mcp-remote',\n",
    "                'https://www.kaggle.com/mcp'\n",
    "            ]\n",
    "        ),\n",
    "        timeout=60,\n",
    "    ),\n",
    "    tool_filter=[\n",
    "        \"search_datasets\", \n",
    "        \"get_dataset_info\", \n",
    "        \"list_dataset_files\",\n",
    "        \"download_dataset\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Data Ingestion Agent: Its job is to find the dataset and its info from kaggle mcp\n",
    "ingest_agent = Agent(\n",
    "    name=\"DataIngestion_agent\",\n",
    "    model=Gemini(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        retry_options=retry_config\n",
    "    ),\n",
    "    instruction=Ingestion_instruction,\n",
    "    tools=[mcp_kaggle_server],\n",
    "    output_key=\"Dataset_files\", \n",
    ")\n",
    "\n",
    "\n",
    "# async def run_ingestion():\n",
    "#     \"\"\"Defines the async context for running the agent.\"\"\"\n",
    "#     runner = InMemoryRunner(agent = ingest_agent)\n",
    "#     response = await runner.run_debug(\"\"\"Find a Kaggle dataset about Netflix movie ratings.\"\"\")\n",
    "#     print(response)\n",
    "\n",
    "# await run_ingestion()\n",
    "\n",
    "print(\"✅ Ingest_agent created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0bfc2f",
   "metadata": {},
   "source": [
    "## 2. Planner Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffc892f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Planner_agent created.\n"
     ]
    }
   ],
   "source": [
    "# Planner Agent: Its job is to use the google_search tool and present findings.\n",
    "Planner_agent = Agent(\n",
    "    name=\"Planner_agent\",\n",
    "    model=Gemini(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        retry_options=retry_config\n",
    "    ),\n",
    "    instruction=Planner_instruction,\n",
    "    tools=[google_search],\n",
    "    output_key=\"Planner_findings\",  # The result of this agent will be stored in the session state with this key.\n",
    ")\n",
    "\n",
    "print(\"✅ Planner_agent created.\")\n",
    "\n",
    "# runner = InMemoryRunner(agent = Planner_agent)\n",
    "# response = await runner.run_debug(Query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6086aa",
   "metadata": {},
   "source": [
    "# DataPilot Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b7b2428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sequential Agent created.\n"
     ]
    }
   ],
   "source": [
    "# # traditional Orchestrator Coordinator: Orchestrates the workflow by calling the sub-agents as tools.\n",
    "# Orchestrator_agent = Agent(\n",
    "#     name=\"OrchestratorCoordinator\",\n",
    "#     model=Gemini(\n",
    "#         model=\"gemini-2.5-flash\",\n",
    "#         retry_options=retry_config\n",
    "#     ),\n",
    "#     instruction=Orchestrator_instruction,\n",
    "#     tools=[AgentTool(ingest_agent), AgentTool(Planner_agent)],\n",
    "# )\n",
    "\n",
    "# print(\"✅ root_agent created.\")\n",
    "\n",
    "# Sequential Orchestrator\n",
    "Orchestrator_agent = SequentialAgent(\n",
    "    name=\"DataSciencePipeline\",\n",
    "    sub_agents=[ingest_agent, Planner_agent],\n",
    ")\n",
    "\n",
    "print(\"✅ Sequential Agent created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1edab661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ### Created new session: debug_session_id\n",
      "\n",
      "User > \n",
      "Find the Breast Cancer Prediction dataset and make a plan to create a classification model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayush/Documents/AI/Projects/GENAI/Datapilot-AI-Agent/Virtual/lib/python3.12/site-packages/google/adk/tools/mcp_tool/mcp_tool.py:101: UserWarning: [EXPERIMENTAL] BaseAuthenticatedTool: This feature is experimental and may change or be removed in future versions without notice. It may introduce breaking changes at any time.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataIngestion_agent > ```json\n",
      "{\n",
      "  \"datasets\": [\n",
      "    {\n",
      "      \"title\": \"Breast Cancer Dataset\",\n",
      "      \"dataset_ref\": \"yasserh/breast-cancer-dataset\",\n",
      "      \"description\": \"### Description:\\n\\nBreast cancer is the most common cancer amongst women in the world. It accounts for 25% of all cancer cases, and affected over 2.1 Million people in 2015 alone. It starts when cells in the breast begin to grow out of control. These cells usually form tumors that can be seen via X-ray or felt as lumps in the breast area.\\n\\nThe key challenges against it’s detection is how to classify tumors into malignant (cancerous) or benign(non cancerous). We ask you to complete the analysis of classifying these tumors using machine learning (with SVMs) and the Breast Cancer Wisconsin (Diagnostic) Dataset.\\n\\n### Acknowledgements:\\nThis dataset has been referred from Kaggle.\\n\\n### Objective:\\n- Understand the Dataset & cleanup (if required).\\n- Build classification models to predict whether the cancer type is Malignant or Benign.\\n- Also fine-tune the hyperparameters & compare the evaluation metrics of various classification algorithms.\",\n",
      "      \"license_name\": \"CC0: Public Domain\",\n",
      "      \"total_bytes\": 124571,\n",
      "      \"tags\": [\n",
      "        \"cancer\",\n",
      "        \"tabular\",\n",
      "        \"classification\",\n",
      "        \"healthcare\",\n",
      "        \"binary classification\"\n",
      "      ],\n",
      "      \"files\": [\n",
      "        {\n",
      "          \"filename\": \"breast-cancer.csv\",\n",
      "          \"size_bytes\": 124571,\n",
      "          \"file_ref\": \"yasserh/breast-cancer-dataset/breast-cancer.csv\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"errors\": []\n",
      "}\n",
      "```\n",
      "Planner_agent > ```json\n",
      "{\n",
      "  \"project_title\": \"Breast Cancer Classification Model Development\",\n",
      "  \"goal\": \"Find the Breast Cancer Prediction dataset and make a plan to create a classification model.\",\n",
      "  \"dataset_reference\": \"yasserh/breast-cancer-dataset\",\n",
      "  \"required_file\": \"breast-cancer.csv\",\n",
      "  \"plan_tasks\": [\n",
      "    {\n",
      "      \"step\": 1,\n",
      "      \"task_name\": \"Data Loading and Initial Inspection\",\n",
      "      \"description\": \"Load the 'breast-cancer.csv' file into a pandas DataFrame. Perform an initial inspection by checking the head of the DataFrame, verifying column data types using `.info()`, and examining basic descriptive statistics using `.describe()` to understand the distribution and potential outliers of numerical features. Specifically, check the status of the target variable 'diagnosis'.\"\n",
      "    },\n",
      "    {\n",
      "      \"step\": 2,\n",
      "      \"task_name\": \"Exploratory Data Analysis (EDA)\",\n",
      "      \"description\": \"Analyze the distribution and class imbalance of the target variable 'diagnosis' (Malignant/Benign). Visualize the distributions of key numerical features such as 'mean_radius', 'mean_texture', 'mean_perimeter', 'mean_area', and 'mean_smoothness'. Examine the correlation matrix between these features and the 'diagnosis' column. Create visualizations like histograms, box plots, and scatter plots to understand relationships and patterns.\"\n",
      "    },\n",
      "    {\n",
      "      \"step\": 3,\n",
      "      \"task_name\": \"Data Cleaning and Preprocessing\",\n",
      "      \"description\": \"Handle any missing or invalid values present in the dataset. Identify and drop or process any irrelevant columns, such as a potential 'id' column. Encode the categorical target variable 'diagnosis' (e.g., 'M' for Malignant to 1, 'B' for Benign to 0). Apply feature scaling (standardization or normalization) to all numerical features, such as 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', and other calculated features, to prepare them for machine learning models, especially distance-based algorithms like Support Vector Machines (SVM).\"\n",
      "    },\n",
      "    {\n",
      "      \"step\": 4,\n",
      "      \"task_name\": \"Feature Engineering and Splitting\",\n",
      "      \"description\": \"Evaluate the possibility of creating new informative features from existing ones (e.g., ratios or interactions of features like 'perimeter_mean' and 'area_mean'). Split the preprocessed dataset into training, validation, and test sets (e.g., 70% training, 15% validation, 15% testing) to ensure a robust evaluation of the model. Ensure that the splitting is stratified based on the 'diagnosis' column to maintain the original class distribution in all subsets.\"\n",
      "    },\n",
      "    {\n",
      "      \"step\": 5,\n",
      "      \"task_name\": \"Model Selection and Training\",\n",
      "      \"description\": \"Select and train appropriate classification models for predicting breast cancer. Given the objective, focus on Support Vector Machines (SVM). Additionally, train other suitable models such as Logistic Regression, Random Forest, or Gradient Boosting classifiers, using the prepared training data. Record the training performance for initial comparison.\"\n",
      "    },\n",
      "    {\n",
      "      \"step\": 6,\n",
      "      \"task_name\": \"Hyperparameter Tuning and Evaluation\",\n",
      "      \"description\": \"Perform hyperparameter tuning for the selected models using techniques like GridSearchCV or RandomizedSearchCV on the training and validation sets. Evaluate the performance of the optimized models on the unseen test set. Key evaluation metrics for this binary classification task should include **Accuracy, Precision, Recall, F1-score, and ROC-AUC** for predicting 'diagnosis' (Malignant/Benign). Compare the performance across different models to determine the best-performing one.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "runner = InMemoryRunner(agent=Orchestrator_agent)\n",
    "response = await runner.run_debug(\"\"\"\n",
    "Find the Breast Cancer Prediction dataset and make a plan to create a classification model.\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
