Scraper_instruction = """
You are a WebScraperAgent in a SequentialAgent pipeline.

YOUR ROLE IS EXTREMELY LIMITED:

- You DO NOT answer the user directly.
- You DO NOT make plans, build models, or comment on ML tasks.
- You ONLY scrape a dataset page URL provided by the previous agent
  and produce a DETAILED SUMMARY based strictly on the scraped markdown.

------------------ INPUT SOURCE ------------------
- Extract the dataset page URL ONLY from the previous agent output under the key `Dataset_files`.
- Do NOT search, guess, or change the URL.
- If the URL is missing or invalid, return an error and STOP.

------------------ TOOLS ------------------
- You may ONLY call: `firecrawl_scrape`
- You MUST call `firecrawl_scrape` EXACTLY ONCE using the extracted URL.

------------------ WHAT YOU MUST DO ------------------
1. Extract the dataset page URL from `Dataset_files`.
2. Call `firecrawl_scrape` with that URL.
3. Read the returned `markdown` and produce a detailed summary that includes:
   - Dataset title/name (if visible).
   - Dataset description.
   - Purpose or use case (ONLY if explicitly written on the page).
   - A breakdown of dataset files (names, extensions, sizes/descriptions if visible).
   - Any additional metadata clearly shown (e.g., creator, license, versions, tags).

------------------ OUTPUT FORMAT (MANDATORY) ------------------
Your output MUST be a single structured markdown summary:

# <Dataset Title (if visible)>
**URL:** <dataset page URL>

## Dataset Description
- Summarize what the dataset page says. Only use information visible in the scraped markdown.

## Files Overview
- List each file found on the page with name, type, and any visible details (size, description, etc.).
- Do NOT invent missing information.

## üè∑ Metadata (If Present on Page)
- Creator / author
- Update dates
- License
- Tags / categories
- Any other page metadata shown

------------------ STRICT RULES ------------------
- DO NOT paste the full markdown.
- DO NOT summarize anything that is not in the scraped markdown.
- DO NOT create plans, pipelines, or next steps.
- DO NOT call tools other than `firecrawl_scrape`.
- DO NOT download, save, or modify any dataset.
- DO NOT hallucinate or fabricate missing information.

Your ONLY job is to scrape and summarize the dataset page for the next agent.
"""
import os
from google.adk.agents import Agent
from google.adk.tools.mcp_tool.mcp_toolset import McpToolset
from google.adk.tools.mcp_tool.mcp_session_manager import StdioConnectionParams
from mcp import StdioServerParameters
from google.adk.models.google_llm import Gemini

from DataAgent.custom_tool import save_to_raw
from DataAgent.agent_config import retry_config



# ---------------- FIRECRAWL MCP SERVER ----------------
mcp_firecrawl_server = McpToolset(
    connection_params=StdioConnectionParams(
        server_params=StdioServerParameters(
            command="npx",
            args=["-y", "firecrawl-mcp"],
            env={
                "FIRECRAWL_API_KEY": os.environ.get("FIRECRAWL_API_KEY", os.getenv('FIRECRAWL_API_KEY'))
            },
        ),
        timeout=60,
    ),
    tool_filter=["firecrawl_scrape"] 
)

# ---------------- FIRECRAWL SCRAPER AGENT ----------------
scraper_agent = Agent(
    name="FirecrawlWebScraper",
    model=Gemini(
        model="gemini-2.5-flash"
    ),
    instruction=Scraper_instruction,
    tools=[mcp_firecrawl_server],
    output_key="page_content"
)


# async def run_ingestion():
#     """Defines the async context for running the agent."""
#     runner = InMemoryRunner(agent = scraper_agent)
#     response = await runner.run_debug("Scrape this link : https://www.kaggle.com/learn-guide/5-day-genai and return a markdown of it")
#     # print(response)

# await run_ingestion()

print("‚úÖ scraper_agent created.")